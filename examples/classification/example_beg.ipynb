{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/paobranco/ImbalanceMetrics.git\n",
      "  Cloning https://github.com/paobranco/ImbalanceMetrics.git to c:\\users\\srtul\\appdata\\local\\temp\\pip-req-build-ah_wy1cu\n",
      "  Resolved https://github.com/paobranco/ImbalanceMetrics.git to commit 190ed1d3a95fd55d37c51cabe2036b99416722e6\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from imbalance-metrics==0.0.5) (1.1.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from imbalance-metrics==0.0.5) (1.23.4)\n",
      "Requirement already satisfied: smogn in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from imbalance-metrics==0.0.5) (0.1.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from imbalance-metrics==0.0.5) (1.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\srtul\\appdata\\roaming\\python\\python38\\site-packages (from pandas->imbalance-metrics==0.0.5) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pandas->imbalance-metrics==0.0.5) (2022.5)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->imbalance-metrics==0.0.5) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->imbalance-metrics==0.0.5) (1.8.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from scikit-learn->imbalance-metrics==0.0.5) (3.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from smogn->imbalance-metrics==0.0.5) (4.64.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->imbalance-metrics==0.0.5) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\srtul\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from tqdm->smogn->imbalance-metrics==0.0.5) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/paobranco/ImbalanceMetrics.git 'C:\\Users\\srtul\\AppData\\Local\\Temp\\pip-req-build-ah_wy1cu'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/paobranco/ImbalanceMetrics.git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImbalanceMetrics (Classification): Usage\n",
    "## Example 1: Beginner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "First, we load the required dependencies. Here we import classification_metrics from imbalance_metrics to evalute the result we get from the SVM. In addition, we use pandas for data handling, and train_test_split to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from imbalance_metrics import classification_metrics as cm\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "After, we load our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0      1     2     3      4     5     6    7    8  9\n",
       "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0  1\n",
       "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0  1\n",
       "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0  1\n",
       "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0  1\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/paobranco/ImbalanceMetrics/main/data/glass.csv', header=None\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign x and y values from the dataframe\n",
    "X = df.drop(columns=[9])\n",
    "y = df[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "After, we train our model with data. In this example, we use the `svm.SVC()` from sklearn. This model will predict as y_pred which we will compare with true test value y in our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(kernel=\"linear\", probability=True, random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "y_proba=clf.predict_proba(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will evaluate the prediction using the functions from classification_metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going calculate the relevance of each class by using `calculate_classification_phi` method. If we set return_phi_per_class as True, the function will only return relevance value of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.045524\n",
       "1    0.077390\n",
       "7    0.103187\n",
       "6    0.773900\n",
       "Name: 9, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi = cm.calculate_classification_phi(y_test, return_phi_per_class = True)\n",
    "phi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set return_phi as default False and phi_option as default 1, the function will return phi relevant weights by class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75     0.045524\n",
       "84     0.045524\n",
       "55     0.077390\n",
       "120    0.045524\n",
       "213    0.103187\n",
       "         ...   \n",
       "90     0.045524\n",
       "15     0.077390\n",
       "174    0.103187\n",
       "30     0.077390\n",
       "107    0.045524\n",
       "Name: 9, Length: 71, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_phi = cm.calculate_classification_phi(y_test) # relevance by class frequency\n",
    "y_phi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set phi_option as 2, the function will return alternative phi relevant weights by class where the phi relevance is divided by class frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75     0.001339\n",
       "84     0.001339\n",
       "55     0.003869\n",
       "120    0.001339\n",
       "213    0.006879\n",
       "         ...   \n",
       "90     0.001339\n",
       "15     0.003869\n",
       "174    0.006879\n",
       "30     0.003869\n",
       "107    0.001339\n",
       "Name: 9, Length: 71, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_phi_alt = cm.calculate_classification_phi(y_test, phi_option = 2)\n",
    "y_phi_alt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have calculated the Phi sample weights, we can pass them as sample_weight in `precision_score`, `recall_score` and `f1_score` etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41860465, 0.68421053, 0.5       , 0.85714286])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred, average=None) # Without any weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49407966, 0.5104712 , 0.94444444, 0.44444444])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, y_pred, average=None, sample_weight = y_phi) # With weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9       , 0.38235294, 0.5       , 0.4       ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred, average=None) # Without any weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9       , 0.38235294, 0.5       , 0.4       ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, y_pred, average=None, sample_weight = y_phi) # With weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57142857, 0.49056604, 0.5       , 0.54545455])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=None) # Without any weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.63794302, 0.43721973, 0.65384615, 0.42105263])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average=None, sample_weight = y_phi) # With weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are using `gmean_score` function to calculate the geometric mean score. First, we are setting weighted as default value True. That means the function will calculate the Phi relevance of each class and use them while calculating the geometric mean score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0663348040214492"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.gmean_score(y_test, y_pred) # Weighted gmean "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set weighted as False the function will only calculate geometric mean score without any weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.512193703259384"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.gmean_score(y_test, y_pred, weighted =False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we are using Precision-Recall AUC using Davis method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will be using default value None as pos_label. When pos_label = None, the minority class is selected as pos_label. In this example, 1 is the minority class so here pos_label = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7083333333333333"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we set default None as positive label so the monority class can be seleted as positive class.\n",
    "cm.pr_davis(y_test, y_proba)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be using value 7 as pos_label. This means 7 will be treated as the positive label for the dataset. This time we will also pass True as return_pr so the function also returns presicion and recall alongside davis auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2238806 , 0.21212121, 0.21538462, 0.21875   , 0.22222222,\n",
       "        0.22580645, 0.2295082 , 0.23333333, 0.23728814, 0.24137931,\n",
       "        0.24561404, 0.25      , 0.25454545, 0.24074074, 0.24528302,\n",
       "        0.25      , 0.23529412, 0.22      , 0.2244898 , 0.22916667,\n",
       "        0.23404255, 0.23913043, 0.24444444, 0.25      , 0.25581395,\n",
       "        0.26190476, 0.26829268, 0.275     , 0.28205128, 0.28947368,\n",
       "        0.2972973 , 0.30555556, 0.31428571, 0.32352941, 0.33333333,\n",
       "        0.34375   , 0.35483871, 0.36666667, 0.37931034, 0.35714286,\n",
       "        0.33333333, 0.34615385, 0.36      , 0.33333333, 0.34782609,\n",
       "        0.36363636, 0.33333333, 0.35      , 0.36842105, 0.38888889,\n",
       "        0.41176471, 0.4375    , 0.46666667, 0.5       , 0.53846154,\n",
       "        0.58333333, 0.63636364, 0.7       , 0.77777778, 0.75      ,\n",
       "        0.85714286, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        ]),\n",
       " array([1.        , 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.93333333, 0.93333333, 0.93333333, 0.86666667, 0.86666667,\n",
       "        0.86666667, 0.8       , 0.73333333, 0.73333333, 0.73333333,\n",
       "        0.73333333, 0.73333333, 0.73333333, 0.73333333, 0.73333333,\n",
       "        0.73333333, 0.73333333, 0.73333333, 0.73333333, 0.73333333,\n",
       "        0.73333333, 0.73333333, 0.73333333, 0.73333333, 0.73333333,\n",
       "        0.73333333, 0.73333333, 0.73333333, 0.73333333, 0.66666667,\n",
       "        0.6       , 0.6       , 0.6       , 0.53333333, 0.53333333,\n",
       "        0.53333333, 0.46666667, 0.46666667, 0.46666667, 0.46666667,\n",
       "        0.46666667, 0.46666667, 0.46666667, 0.46666667, 0.46666667,\n",
       "        0.46666667, 0.46666667, 0.46666667, 0.46666667, 0.4       ,\n",
       "        0.4       , 0.4       , 0.33333333, 0.26666667, 0.2       ,\n",
       "        0.13333333, 0.06666667, 0.        ]),\n",
       " 0.6072295480081297)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we set 0 as positive label\n",
    "cm.pr_davis(y_test, y_proba,return_pr=True,pos_label=7)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Davis method, we are using Precision-Recall AUC using Manning method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Davis method, we will be using default value None as pos_label first. This means minority class 6 will be treated as the positive label for the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we set default None as positive label so the monority class can be seleted as positive class.\n",
    "cm.pr_manning(y_test, y_proba)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be using value 7 as pos_label. This means 7 will be treated as the positive label for the dataset. This time we will also pass True as return_pr so the function also returns presicion and recall alongside manning auc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.85714286, 0.77777778, 0.77777778,\n",
       "        0.7       , 0.63636364, 0.58333333, 0.53846154, 0.5       ,\n",
       "        0.46666667, 0.4375    , 0.41176471, 0.38888889, 0.37931034,\n",
       "        0.37931034, 0.37931034, 0.37931034, 0.37931034, 0.37931034,\n",
       "        0.37931034, 0.37931034, 0.37931034, 0.37931034, 0.37931034,\n",
       "        0.36666667, 0.35483871, 0.34375   , 0.33333333, 0.32352941,\n",
       "        0.31428571, 0.30555556, 0.2972973 , 0.28947368, 0.28205128,\n",
       "        0.275     , 0.26829268, 0.26190476, 0.25581395, 0.25454545,\n",
       "        0.25454545, 0.25454545, 0.25454545, 0.25454545, 0.25454545,\n",
       "        0.25454545, 0.25454545, 0.25454545, 0.25454545, 0.25454545,\n",
       "        0.25454545, 0.25      , 0.24561404, 0.24137931, 0.23728814,\n",
       "        0.23333333, 0.2295082 , 0.22580645, 0.2238806 , 0.2238806 ,\n",
       "        0.2238806 , 0.2238806 , 0.2238806 , 0.22058824, 0.2173913 ,\n",
       "        0.21428571, 0.21126761]),\n",
       " array([0.        , 0.06666667, 0.13333333, 0.2       , 0.26666667,\n",
       "        0.33333333, 0.4       , 0.4       , 0.4       , 0.46666667,\n",
       "        0.46666667, 0.46666667, 0.46666667, 0.46666667, 0.46666667,\n",
       "        0.46666667, 0.46666667, 0.46666667, 0.46666667, 0.46666667,\n",
       "        0.46666667, 0.46666667, 0.53333333, 0.53333333, 0.53333333,\n",
       "        0.6       , 0.6       , 0.6       , 0.66666667, 0.73333333,\n",
       "        0.73333333, 0.73333333, 0.73333333, 0.73333333, 0.73333333,\n",
       "        0.73333333, 0.73333333, 0.73333333, 0.73333333, 0.73333333,\n",
       "        0.73333333, 0.73333333, 0.73333333, 0.73333333, 0.73333333,\n",
       "        0.73333333, 0.73333333, 0.73333333, 0.73333333, 0.73333333,\n",
       "        0.73333333, 0.8       , 0.86666667, 0.86666667, 0.86666667,\n",
       "        0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.93333333, 0.93333333, 0.93333333, 0.93333333, 0.93333333,\n",
       "        0.93333333, 0.93333333, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        ]),\n",
       " 0.6188357411826275)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we set 0 as positive label\n",
    "cm.pr_manning(y_test, y_proba,return_pr=True,pos_label=7)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this package, we have presented a set of evaluation metrics specifically designed for imbalanced domains. Our package, \"ImbalanceMetrics\", provides a comprehensive set of evaluation metrics to assess the performance of machine learning models trained on imbalanced datasets.\n",
    "\n",
    "Our package includes several evaluation metrics that address the challenges of imbalanced domains. These metrics can provide a more accurate assessment of model performance than traditional metrics, which can be misleading in imbalanced domains.\n",
    "\n",
    "To learn more about our package, please refer to the documentation, which includes detailed descriptions of all the available metrics and their usage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "94a785af1193d76d93209509f96efa5fea05cc7bc5e0b97b40dbaa0c7dd300a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
